{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "- Markov chains are a network of states which can be converted to and from one another.\n",
    "- Being in one state, there is may be 5 other states which you could possibly end up next, there is a probability for you to end up in each of those states.\n",
    "- This probability is called the transition probability - the probability for you to transition from State A to State B.\n",
    "- Importantly, the probability for you to move to one state is completely indepdent of your past, it is memorylessness. \n",
    "- For example, the probability for you to move to State B given you are on State A can be 0.3. This 0.3 value is static and will not change regardless of which states you have landed on in the past.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary distribution\n",
    "- By running through the Markov chain continuously moving from one state to the next, we can record how often we land on each state as we transition from state to state.\n",
    "- Particularly, there is an interesting property where the distribution of time we spend on each state will reach a stationary constant given enough attempts.\n",
    "- This feature is occurs regardless of where you start out with, eventually, due to the probabilities of each state transition, we will converge on a constant proportion of time we spend on each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markkov decision processes are very similar to Markov chains, to transition from one state to the next, the individual needs to take an action, and it is the combination of the state and action that results in the transition probabilities to moving to the possible states.\n",
    "- Interesting addition is that there is a reward at every state, the key to Markov decision processes is to optimise for the reward.\n",
    "- We know from Markov chains that we can calculate the stationary distribution based on the transition probabilities. If we know how many times we would land on each state, we then can calculate the expected reward that we would get.\n",
    "- Another addition is Policy, policy is just an internal rule to help you decide which action you should take. In Markov decision processes, there is a choice to be made, a choice to choose from a number of possible actions. The combination of action and state then results in transitional probabilities.\n",
    "- Ideally, you would want to optimise your policy such that you maximise utility, which is the expected reward you would receive if you run your policy enough times.\n",
    "- In order to optimise your policy, you need a way to calculate the rewards attained from such a policy. This reward can be calculative via a recursive function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "V_{\\prod_{}^{}}(s)=\\left\\{\n",
    "    \\begin{array}{cl}\n",
    "        0, \\text{ if at end state} \\\\\n",
    "        \\sum_{s'}^{} T(s,a,s')[R(s, a, s') + \\gamma V_{\\prod_{}^{}}(s') ]\n",
    "    \\end{array}\n",
    "\\right\\}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be more simply written in code via a recursive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_next_states(state, action, discount_factor = 1):\n",
    "    return []\n",
    "\n",
    "def calc_reward(state, action):\n",
    "    if state == \"end\":\n",
    "        return 0\n",
    "    else:\n",
    "        possible_next_states = get_possible_next_states(state, action)\n",
    "        total_reward = 0\n",
    "        for next_state in possible_next_states:\n",
    "            next_state_reward = next_state.reward\n",
    "            future_states_reward = calc_reward(next_state, action, discount_factor * discount_factor)\n",
    "\n",
    "            total_reward += (next_state.prob * (next_state_reward + future_states_reward))\n",
    "            \n",
    "        return total_reward\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state():\n",
    "    pass\n",
    "\n",
    "    \n",
    "\n",
    "def calc_reward(state, action, curr_reward, prev_reward, threshold):\n",
    "    if (curr_iter - prev_reward) > threshold:\n",
    "        return curr_reward\n",
    "    \n",
    "    next_state = get_next_state()\n",
    "\n",
    "    return calc_reward(next_state, action, curr_reward + state.reward, curr_reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from dice game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice_reward(prev_iter_reward = 0):\n",
    "    end_state_reward = 0\n",
    "    in_state_reward = 1/3 * (4 + end_state_reward) + 2/3 * (4 + prev_iter_reward)\n",
    "    return calc_dice_reward(prev_iter_reward= end_state_reward + in_state_reward)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
